# -*- coding: utf-8 -*-
"""Wireless Mobile Comms.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yl8tHkAnjmNhfwxpkVbKUiqCeJy6s_B8

# **Importing the library**
"""

import pandas as pd
import matplotlib.pyplot as plt
import folium
import plotly.express as px

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

"""# **Loading the dataset**"""

sample_data = pd.read_csv("signal_metrics_updated.csv")
print(sample_data.head())

"""# **Fetching the relevant data from the data**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
import folium
from folium.plugins import HeatMap
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import scipy.stats as stats
from scipy.spatial.distance import pdist, squareform

warnings.filterwarnings('ignore')

class MobileNetworkAnalyzer:
    def __init__(self):
        self.data = None
        self.processed_data = None

    def load_data(self, file_path=None, sample_data=False):
        """Load dataset from file or generate sample data for demonstration"""
        if sample_data or file_path is None:
            np.random.seed(42)
            n_samples = 1000
            start_date = datetime(2024, 1, 1)
            timestamps = [start_date + timedelta(hours=i/10) for i in range(n_samples)]


            localities = ['Downtown', 'Suburban', 'Rural', 'Industrial', 'Residential']


            base_coords = {
                'Downtown': (40.7589, -73.9851),
                'Suburban': (40.7505, -73.9934),
                'Rural': (40.6892, -74.0445),
                'Industrial': (40.7282, -74.0776),
                'Residential': (40.7831, -73.9712)
            }

            data = []
            for i in range(n_samples):
                locality = np.random.choice(localities)
                base_lat, base_lon = base_coords[locality]


                lat = base_lat + np.random.normal(0, 0.01)
                lon = base_lon + np.random.normal(0, 0.01)


                if locality == 'Downtown':
                    signal_strength = np.random.normal(-65, 10)
                    signal_quality = np.random.normal(85, 10)
                    throughput = np.random.normal(50, 15)
                    latency = np.random.normal(20, 8)
                elif locality == 'Rural':
                    signal_strength = np.random.normal(-85, 15)
                    signal_quality = np.random.normal(65, 15)
                    throughput = np.random.normal(25, 10)
                    latency = np.random.normal(35, 12)
                else:
                    signal_strength = np.random.normal(-75, 12)
                    signal_quality = np.random.normal(75, 12)
                    throughput = np.random.normal(40, 12)
                    latency = np.random.normal(25, 10)


                signal_strength = np.clip(signal_strength, -120, -30)
                signal_quality = np.clip(signal_quality, 0, 100)
                throughput = np.clip(throughput, 1, 100)
                latency = np.clip(latency, 5, 100)

                network_type = np.random.choice(['4G', '5G'], p=[0.7, 0.3])


                bb60c = signal_strength + np.random.normal(0, 2)
                srsran = signal_strength + np.random.normal(0, 3)
                bladerf = signal_strength + np.random.normal(0, 2.5)

                data.append([
                    timestamps[i], locality, lat, lon, signal_strength, signal_quality,
                    throughput, latency, network_type, bb60c, srsran, bladerf
                ])

            columns = [
                'Timestamp', 'Locality', 'Latitude', 'Longitude', 'Signal Strength (dBm)',
                'Signal Quality (%)', 'Data Throughput (Mbps)', 'Latency (ms)', 'Network Type',
                'BB60C Measurement (dBm)', 'srsRAN Measurement (dBm)', 'BladeRFxA9 Measurement (dBm)'
            ]

            self.data = pd.DataFrame(data, columns=columns)
        else:

            self.data = pd.read_csv(file_path)

        print(f"Data loaded successfully! Shape: {self.data.shape}")
        return self.data.head()

    def preprocess_data(self):
        """Clean and preprocess the data"""
        self.processed_data = self.data.copy()


        self.processed_data['Timestamp'] = pd.to_datetime(self.processed_data['Timestamp'])


        self.processed_data['Hour'] = self.processed_data['Timestamp'].dt.hour
        self.processed_data['DayOfWeek'] = self.processed_data['Timestamp'].dt.day_name()
        self.processed_data['Month'] = self.processed_data['Timestamp'].dt.month


        self.processed_data['Coverage_Quality_Score'] = (
            (self.processed_data['Signal Strength (dBm)'] + 120) / 90 * 50 +
            self.processed_data['Signal Quality (%)'] / 100 * 50
        )


        self.processed_data['QoS_Score'] = (
            self.processed_data['Data Throughput (Mbps)'] / 100 * 40 +
            (100 - self.processed_data['Latency (ms)']) / 100 * 30 +
            self.processed_data['Signal Quality (%)'] / 100 * 30
        )


        instrument_cols = ['BB60C Measurement (dBm)', 'srsRAN Measurement (dBm)', 'BladeRFxA9 Measurement (dBm)']
        self.processed_data['Avg_Instrument_Measurement'] = self.processed_data[instrument_cols].mean(axis=1)

        print("Data preprocessing completed!")
        return self.processed_data.describe()

    def analyze_coverage_statistics(self):
        """Analyze coverage statistics by locality and network type"""
        print("=== COVERAGE ANALYSIS ===")


        locality_stats = self.processed_data.groupby('Locality').agg({
            'Signal Strength (dBm)': ['mean', 'std', 'min', 'max'],
            'Signal Quality (%)': ['mean', 'std'],
            'Coverage_Quality_Score': ['mean', 'std']
        }).round(2)

        print("\nüìç Coverage Statistics by Locality:")
        print(locality_stats)


        network_stats = self.processed_data.groupby('Network Type').agg({
            'Signal Strength (dBm)': 'mean',
            'Signal Quality (%)': 'mean',
            'Data Throughput (Mbps)': 'mean',
            'Latency (ms)': 'mean',
            'QoS_Score': 'mean'
        }).round(2)

        print("\nüì∂ Performance by Network Type:")
        print(network_stats)

        return locality_stats, network_stats

    def analyze_qos_performance(self):
        """Analyze Quality of Service metrics"""
        print("\n=== QoS ANALYSIS ===")


        def categorize_qos(score):
            if score >= 75: return 'Excellent'
            elif score >= 60: return 'Good'
            elif score >= 45: return 'Fair'
            else: return 'Poor'

        self.processed_data['QoS_Category'] = self.processed_data['QoS_Score'].apply(categorize_qos)


        qos_distribution = self.processed_data['QoS_Category'].value_counts(normalize=True) * 100
        print("\nüéØ QoS Distribution:")
        for category, percentage in qos_distribution.items():
            print(f"  {category}: {percentage:.1f}%")


        correlation_cols = ['Signal Strength (dBm)', 'Signal Quality (%)',
                          'Data Throughput (Mbps)', 'Latency (ms)', 'QoS_Score']
        correlation_matrix = self.processed_data[correlation_cols].corr()

        print("\nüîó Key Correlations with QoS Score:")
        qos_correlations = correlation_matrix['QoS_Score'].drop('QoS_Score').sort_values(key=abs, ascending=False)
        for metric, corr in qos_correlations.items():
            print(f"  {metric}: {corr:.3f}")

        return qos_distribution, correlation_matrix

    def instrument_comparison_analysis(self):
        """Compare measurements from different instruments"""
        print("\n=== INSTRUMENT COMPARISON ===")

        instrument_cols = ['BB60C Measurement (dBm)', 'srsRAN Measurement (dBm)', 'BladeRFxA9 Measurement (dBm)']


        for i, col1 in enumerate(instrument_cols):
            for j, col2 in enumerate(instrument_cols[i+1:], i+1):
                diff_col = f"{col1.split()[0]}_vs_{col2.split()[0]}_diff"
                self.processed_data[diff_col] = self.processed_data[col1] - self.processed_data[col2]


        print("\nüìä Instrument Measurement Statistics:")
        instrument_stats = self.processed_data[instrument_cols].describe().round(2)
        print(instrument_stats)


        instrument_corr = self.processed_data[instrument_cols].corr()
        print("\nüîÑ Inter-instrument Correlations:")
        print(instrument_corr.round(3))

        return instrument_stats, instrument_corr

    def temporal_analysis(self):
        """Analyze temporal patterns in network performance"""
        print("\n=== TEMPORAL ANALYSIS ===")


        hourly_performance = self.processed_data.groupby('Hour').agg({
            'Data Throughput (Mbps)': 'mean',
            'Latency (ms)': 'mean',
            'QoS_Score': 'mean'
        }).round(2)

        print("\n‚è∞ Peak Performance Hours:")
        best_hours = hourly_performance.nlargest(3, 'QoS_Score')
        worst_hours = hourly_performance.nsmallest(3, 'QoS_Score')

        print("Best Performance Hours:")
        print(best_hours)
        print("\nWorst Performance Hours:")
        print(worst_hours)


        daily_performance = self.processed_data.groupby('DayOfWeek').agg({
            'Data Throughput (Mbps)': 'mean',
            'Latency (ms)': 'mean',
            'QoS_Score': 'mean'
        }).round(2)

        print("\nüìÖ Performance by Day of Week:")
        print(daily_performance.sort_values('QoS_Score', ascending=False))

        return hourly_performance, daily_performance

    def create_visualizations(self):
        """Create comprehensive visualizations"""
        plt.style.use('default')
        fig = plt.figure(figsize=(20, 15))


        plt.subplot(3, 4, 1)
        sns.boxplot(data=self.processed_data, x='Locality', y='Signal Strength (dBm)')
        plt.title('Signal Strength by Locality', fontweight='bold')
        plt.xticks(rotation=45)


        plt.subplot(3, 4, 2)
        plt.hist(self.processed_data['QoS_Score'], bins=30, alpha=0.7, color='skyblue')
        plt.title('QoS Score Distribution', fontweight='bold')
        plt.xlabel('QoS Score')
        plt.ylabel('Frequency')


        plt.subplot(3, 4, 3)
        network_comparison = self.processed_data.groupby('Network Type')[['Data Throughput (Mbps)', 'Latency (ms)']].mean()
        network_comparison['Data Throughput (Mbps)'].plot(kind='bar', color='green', alpha=0.7)
        plt.title('Throughput by Network Type', fontweight='bold')
        plt.ylabel('Throughput (Mbps)')
        plt.xticks(rotation=0)


        plt.subplot(3, 4, 4)
        correlation_cols = ['Signal Strength (dBm)', 'Signal Quality (%)', 'Data Throughput (Mbps)', 'Latency (ms)']
        corr_matrix = self.processed_data[correlation_cols].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('Performance Metrics Correlation', fontweight='bold')


        plt.subplot(3, 4, 5)
        hourly_qos = self.processed_data.groupby('Hour')['QoS_Score'].mean()
        hourly_qos.plot(kind='line', marker='o', color='purple')
        plt.title('QoS Score Throughout the Day', fontweight='bold')
        plt.xlabel('Hour of Day')
        plt.ylabel('Average QoS Score')
        plt.grid(True, alpha=0.3)


        plt.subplot(3, 4, 6)
        instrument_cols = ['BB60C Measurement (dBm)', 'srsRAN Measurement (dBm)', 'BladeRFxA9 Measurement (dBm)']
        instrument_means = self.processed_data[instrument_cols].mean()
        instrument_means.plot(kind='bar', color=['red', 'blue', 'green'], alpha=0.7)
        plt.title('Average Instrument Measurements', fontweight='bold')
        plt.ylabel('Signal Strength (dBm)')
        plt.xticks(rotation=45)


        plt.subplot(3, 4, 7)
        sns.violinplot(data=self.processed_data, x='Locality', y='Coverage_Quality_Score')
        plt.title('Coverage Quality by Locality', fontweight='bold')
        plt.xticks(rotation=45)


        plt.subplot(3, 4, 8)
        scatter = plt.scatter(self.processed_data['Data Throughput (Mbps)'],
                            self.processed_data['Latency (ms)'],
                            c=self.processed_data['Signal Strength (dBm)'],
                            cmap='viridis', alpha=0.6)
        plt.colorbar(scatter, label='Signal Strength (dBm)')
        plt.xlabel('Data Throughput (Mbps)')
        plt.ylabel('Latency (ms)')
        plt.title('Throughput vs Latency', fontweight='bold')


        plt.subplot(3, 4, 9)
        daily_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        daily_qos = self.processed_data.groupby('DayOfWeek')['QoS_Score'].mean()
        daily_qos_ordered = daily_qos.reindex(daily_order)
        daily_qos_ordered.plot(kind='bar', color='orange', alpha=0.7)
        plt.title('QoS Score by Day of Week', fontweight='bold')
        plt.ylabel('Average QoS Score')
        plt.xticks(rotation=45)


        plt.subplot(3, 4, 10)
        plt.scatter(self.processed_data['Signal Strength (dBm)'],
                   self.processed_data['Signal Quality (%)'],
                   alpha=0.6, color='teal')
        plt.xlabel('Signal Strength (dBm)')
        plt.ylabel('Signal Quality (%)')
        plt.title('Signal Strength vs Quality', fontweight='bold')


        plt.subplot(3, 4, 11)
        qos_counts = self.processed_data['QoS_Category'].value_counts()
        plt.pie(qos_counts.values, labels=qos_counts.index, autopct='%1.1f%%')
        plt.title('QoS Category Distribution', fontweight='bold')


        plt.subplot(3, 4, 12)
        for i, col in enumerate(instrument_cols):
            plt.hist(self.processed_data[col], bins=20, alpha=0.5,
                    label=col.split()[0], color=['red', 'blue', 'green'][i])
        plt.legend()
        plt.title('Instrument Measurement Distribution', fontweight='bold')
        plt.xlabel('Signal Strength (dBm)')
        plt.ylabel('Frequency')

        plt.tight_layout()
        plt.show()

    def create_coverage_map(self):
        """Create an interactive coverage map"""
        try:

            center_lat = self.processed_data['Latitude'].mean()
            center_lon = self.processed_data['Longitude'].mean()


            m = folium.Map(location=[center_lat, center_lon], zoom_start=11)


            heat_data = [[row['Latitude'], row['Longitude'], row['Signal Strength (dBm)']]
                        for idx, row in self.processed_data.iterrows()]


            HeatMap(heat_data, radius=15, blur=10, max_zoom=1).add_to(m)


            locality_centers = self.processed_data.groupby('Locality')[['Latitude', 'Longitude', 'QoS_Score']].mean()

            for locality, data in locality_centers.iterrows():
                color = 'green' if data['QoS_Score'] > 60 else 'orange' if data['QoS_Score'] > 45 else 'red'
                folium.Marker(
                    [data['Latitude'], data['Longitude']],
                    popup=f"{locality}<br>Avg QoS: {data['QoS_Score']:.1f}",
                    tooltip=locality,
                    icon=folium.Icon(color=color)
                ).add_to(m)


            m.save('coverage_map.html')
            print("üìç Interactive coverage map saved as 'coverage_map.html'")

            return m
        except Exception as e:
            print(f"Could not create map: {e}")
            return None

    def predictive_modeling(self):
        """Build predictive models for QoS and throughput"""
        print("\n=== PREDICTIVE MODELING ===")


        le = LabelEncoder()
        model_data = self.processed_data.copy()
        model_data['Locality_encoded'] = le.fit_transform(model_data['Locality'])
        model_data['Network_Type_encoded'] = LabelEncoder().fit_transform(model_data['Network Type'])


        feature_cols = ['Signal Strength (dBm)', 'Signal Quality (%)', 'Latitude', 'Longitude',
                       'Hour', 'Locality_encoded', 'Network_Type_encoded', 'Avg_Instrument_Measurement']

        X = model_data[feature_cols]


        y_throughput = model_data['Data Throughput (Mbps)']
        X_train, X_test, y_train, y_test = train_test_split(X, y_throughput, test_size=0.2, random_state=42)

        throughput_model = RandomForestRegressor(n_estimators=100, random_state=42)
        throughput_model.fit(X_train, y_train)

        throughput_pred = throughput_model.predict(X_test)
        throughput_r2 = r2_score(y_test, throughput_pred)
        throughput_mse = mean_squared_error(y_test, throughput_pred)

        print(f"\nüéØ Throughput Prediction Model:")
        print(f"  R¬≤ Score: {throughput_r2:.3f}")
        print(f"  RMSE: {np.sqrt(throughput_mse):.3f} Mbps")


        y_qos = model_data['QoS_Score']
        X_train, X_test, y_train, y_test = train_test_split(X, y_qos, test_size=0.2, random_state=42)

        qos_model = RandomForestRegressor(n_estimators=100, random_state=42)
        qos_model.fit(X_train, y_train)

        qos_pred = qos_model.predict(X_test)
        qos_r2 = r2_score(y_test, qos_pred)
        qos_mse = mean_squared_error(y_test, qos_pred)

        print(f"\nüéØ QoS Prediction Model:")
        print(f"  R¬≤ Score: {qos_r2:.3f}")
        print(f"  RMSE: {np.sqrt(qos_mse):.3f}")


        print(f"\nüîç Top Features for Throughput Prediction:")
        throughput_importance = pd.DataFrame({
            'Feature': feature_cols,
            'Importance': throughput_model.feature_importances_
        }).sort_values('Importance', ascending=False)

        for idx, row in throughput_importance.head(5).iterrows():
            print(f"  {row['Feature']}: {row['Importance']:.3f}")

        return throughput_model, qos_model, throughput_importance

    def generate_recommendations(self):
        """Generate actionable recommendations"""
        print("\n=== NETWORK OPTIMIZATION RECOMMENDATIONS ===")


        poor_coverage_areas = self.processed_data[self.processed_data['QoS_Score'] < 45]['Locality'].value_counts()
        high_latency_areas = self.processed_data[self.processed_data['Latency (ms)'] > 40]['Locality'].value_counts()
        low_throughput_areas = self.processed_data[self.processed_data['Data Throughput (Mbps)'] < 20]['Locality'].value_counts()

        recommendations = []


        if len(poor_coverage_areas) > 0:
            recommendations.append(f"üö® PRIORITY: Improve coverage in {', '.join(poor_coverage_areas.head(3).index)}")
            recommendations.append(f"   - Consider additional base stations or signal boosters")
            recommendations.append(f"   - Check for interference sources")


        if len(high_latency_areas) > 0:
            recommendations.append(f"‚ö° LATENCY: Address high latency in {', '.join(high_latency_areas.head(2).index)}")
            recommendations.append(f"   - Optimize routing and backhaul connections")
            recommendations.append(f"   - Consider edge computing deployment")


        if len(low_throughput_areas) > 0:
            recommendations.append(f"üìä CAPACITY: Increase capacity in {', '.join(low_throughput_areas.head(2).index)}")
            recommendations.append(f"   - Upgrade to higher bandwidth infrastructure")
            recommendations.append(f"   - Implement load balancing")


        hourly_qos = self.processed_data.groupby('Hour')['QoS_Score'].mean()
        worst_hours = hourly_qos.nsmallest(3)
        recommendations.append(f"‚è∞ PEAK HOURS: Focus on hours {', '.join(map(str, worst_hours.index))}")
        recommendations.append(f"   - Implement dynamic resource allocation")
        recommendations.append(f"   - Consider peak-hour pricing or traffic shaping")


        network_performance = self.processed_data.groupby('Network Type')['QoS_Score'].mean()
        if '4G' in network_performance.index and '5G' in network_performance.index:
            if network_performance['5G'] > network_performance['4G']:
                recommendations.append(f"üì∂ TECHNOLOGY: Accelerate 5G deployment")
                recommendations.append(f"   - 5G shows {network_performance['5G'] - network_performance['4G']:.1f} point QoS improvement")

        print("\nüìã Recommendations:")
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")

        return recommendations

    def export_results(self):
        """Export analysis results to files"""
        try:

            self.processed_data.to_csv('processed_network_data.csv', index=False)


            summary_stats = {
                'Total_Measurements': len(self.processed_data),
                'Average_Signal_Strength': self.processed_data['Signal Strength (dBm)'].mean(),
                'Average_QoS_Score': self.processed_data['QoS_Score'].mean(),
                'Coverage_Areas': self.processed_data['Locality'].nunique(),
                'Network_Types': ', '.join(self.processed_data['Network Type'].unique())
            }

            with open('network_analysis_summary.txt', 'w') as f:
                f.write("Mobile Network Coverage & QoS Analysis Summary\n")
                f.write("=" * 50 + "\n\n")
                for key, value in summary_stats.items():
                    f.write(f"{key.replace('_', ' ')}: {value}\n")

            print("\nüíæ Results exported:")
            print("  - processed_network_data.csv")
            print("  - network_analysis_summary.txt")
            print("  - coverage_map.html (if map was created)")

        except Exception as e:
            print(f"Export error: {e}")

    def run_complete_analysis(self, file_path=None):
        """Run the complete analysis pipeline"""
        print("üöÄ Starting Mobile Network Coverage & QoS Analysis...")
        print("=" * 60)


        self.load_data(file_path, sample_data=(file_path is None))
        self.preprocess_data()


        self.analyze_coverage_statistics()
        self.analyze_qos_performance()
        self.instrument_comparison_analysis()
        self.temporal_analysis()


        print("\nüìä Creating visualizations...")
        self.create_visualizations()


        print("\nüó∫Ô∏è Creating coverage map...")
        self.create_coverage_map()


        self.predictive_modeling()


        self.generate_recommendations()


        self.export_results()

        print("\n‚úÖ Analysis completed successfully!")
        print("Check the generated files for detailed results and visualizations.")


if __name__ == "__main__":

    analyzer = MobileNetworkAnalyzer()


    analyzer.run_complete_analysis()

    print("\n" + "="*60)
    print("üéâ Mobile Network Coverage & QoS Analysis Complete!")
    print("="*60)


    if analyzer.processed_data is not None:
        print("\nüìä Quick Stats:")
        print(f"Total measurements: {len(analyzer.processed_data)}")
        print(f"Average QoS Score: {analyzer.processed_data['QoS_Score'].mean():.2f}")
        print(f"Best performing locality: {analyzer.processed_data.groupby('Locality')['QoS_Score'].mean().idxmax()}")
        print(f"Network coverage: {analyzer.processed_data['Locality'].nunique()} areas analyzed")

df = pd.read_csv("signal_metrics.csv")
df.head()

"""Fetching the Stats from the dataset"""

print("\n--- Basic QoS Stats ---")
print("Average Signal Strength (dBm):", df['Signal Strength (dBm)'].mean())
print("Average Signal Quality (%):", df['Signal Quality (%)'].mean())
print("Average Throughput (Mbps):", df['Data Throughput (Mbps)'].mean())
print("Average Latency (ms):", df['Latency (ms)'].mean())
print("\n--- Measurement Device Stats ---")
print("BB60C Avg (dBm):", df['BB60C Measurement (dBm)'].mean())
print("srsRAN Avg (dBm):", df['srsRAN Measurement (dBm)'].mean())
print("BladeRFxA9 Avg (dBm):", df['BladeRFxA9 Measurement (dBm)'].mean())

#Signal Strength Over Time
plt.figure(figsize=(12,6))
plt.plot(df['Timestamp'], df['Signal Strength (dBm)'], marker='o', label="Signal Strength")
plt.title("Signal Strength Over Time")
plt.xlabel("Timestamp")
plt.ylabel("Signal Strength (dBm)")
plt.xticks(rotation=45)
plt.grid(True)
plt.legend()
plt.show()

#Latency vs Throughput
plt.figure(figsize=(7,5))
plt.scatter(df['Latency (ms)'], df['Data Throughput (Mbps)'],
            c=df['Signal Strength (dBm)'], cmap="coolwarm", s=80)
plt.colorbar(label="Signal Strength (dBm)")
plt.title("Latency vs Data Throughput")
plt.xlabel("Latency (ms)")
plt.ylabel("Data Throughput (Mbps)")
plt.grid(True)
plt.show()

#Coverage Map (Folium)
m = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=12)

for _, row in df.iterrows():
    # Color code by signal quality
    if row['Signal Quality (%)'] > 70:
        color = "green"
    elif row['Signal Quality (%)'] > 40:
        color = "orange"
    else:
        color = "red"

    folium.CircleMarker(
        location=[row['Latitude'], row['Longitude']],
        radius=6,
        popup=(
            f"Locality: {row['Locality']}<br>"
            f"Signal: {row['Signal Strength (dBm)']} dBm<br>"
            f"Quality: {row['Signal Quality (%)']}%<br>"
            f"Throughput: {row['Data Throughput (Mbps)']} Mbps<br>"
            f"Latency: {row['Latency (ms)']} ms<br>"
            f"Network: {row['Network Type']}<br>"
            f"BB60C: {row['BB60C Measurement (dBm)']} dBm<br>"
            f"srsRAN: {row['srsRAN Measurement (dBm)']} dBm<br>"
            f"BladeRFxA9: {row['BladeRFxA9 Measurement (dBm)']} dBm"
        ),
        color=color,
        fill=True,
        fill_color=color
    ).add_to(m)

m.save("coverage_map.html")
print("\n‚úÖ Coverage map saved as coverage_map.html")

fig = px.density_mapbox(
    df,
    lat="Latitude",
    lon="Longitude",
    z="Signal Strength (dBm)",   # heat intensity
    radius=20,
    center=dict(lat=df['Latitude'].mean(), lon=df['Longitude'].mean()),
    zoom=12,
    mapbox_style="open-street-map",   # ‚úÖ works without token
    color_continuous_scale="Viridis",
    title="Signal Strength Heatmap"
)

fig.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import lightgbm as lgb

# -------------------------
# Step 1: Feature Engineering
# -------------------------
df["Device_Avg_dBm"] = df[["BB60C Measurement (dBm)",
                           "srsRAN Measurement (dBm)",
                           "BladeRFxA9 Measurement (dBm)"]].mean(axis=1)

df["Signal_Latency_Ratio"] = df["Signal Strength (dBm)"] / (df["Latency (ms)"]+1)
df["Quality_Signal"] = df["Signal Quality (%)"] * df["Signal Strength (dBm)"]
df["Signal_Strength_Sq"] = df["Signal Strength (dBm)"]**2
df["Latency_Sq"] = df["Latency (ms)"]**2

# -------------------------
# Step 2: Features + Target
# -------------------------
X = df[[
    "Signal Strength (dBm)",
    "Signal Quality (%)",
    "Latency (ms)",
    "BB60C Measurement (dBm)",
    "srsRAN Measurement (dBm)",
    "BladeRFxA9 Measurement (dBm)",
    "Device_Avg_dBm",
    "Signal_Latency_Ratio",
    "Quality_Signal",
    "Signal_Strength_Sq",
    "Latency_Sq"
]]
y = df["Data Throughput (Mbps)"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -------------------------
# Step 3: LightGBM + GridSearch
# -------------------------
param_grid = {
    "num_leaves": [31, 50, 80],
    "learning_rate": [0.01, 0.05, 0.1],
    "n_estimators": [200, 500, 800],
    "max_depth": [6, 10, -1]
}

lgb_model = lgb.LGBMRegressor(random_state=42)

grid = GridSearchCV(lgb_model, param_grid, cv=3, scoring="r2", n_jobs=-1, verbose=1)
grid.fit(X_train, y_train)

# -------------------------
# Step 4: Best Model
# -------------------------
best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

print("\n‚úÖ Best Params:", grid.best_params_)
print("R¬≤ Score:", r2_score(y_test, y_pred))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

import matplotlib.pyplot as plt

feat_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": best_model.feature_importances_
}).sort_values(by="Importance", ascending=False)

plt.figure(figsize=(8,5))
plt.barh(feat_importance["Feature"], feat_importance["Importance"], color="teal")
plt.title("Feature Importance (LightGBM)")
plt.xlabel("Relative Importance")
plt.show()

"""
End-to-end pipeline for Mobile Network Coverage & QoS analysis
- Assumes dataset at /mnt/data/signal_metrics.csv
- Outputs cleaned CSV and a short report into /mnt/data/ml_output/
- Produces matplotlib visualizations and trains a simple classifier
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix



DATA_PATH = "/mnt/data/signal_metrics.csv"
OUT_DIR = "/mnt/data/ml_output"
os.makedirs(OUT_DIR, exist_ok=True)

# -----------------------
# 1) Load dataset
# -----------------------
df = pd.read_csv("signal_metrics_cleaned.csv")
print("Loaded:", df.shape)
print("Columns:", df.columns.tolist())
print(df.head(5))

# -----------------------
# 2) Detect / map columns (adapt to your CSV)
# -----------------------
cols_lower = {c.lower(): c for c in df.columns}

def pick(*options):
    for o in options:
        if o.lower() in cols_lower:
            return cols_lower[o.lower()]
    return None

timestamp_col = pick('timestamp', 'time', 'date')
lat_col       = pick('latitude', 'lat', 'gps_lat')
lon_col       = pick('longitude', 'lon', 'gps_lon')
rsrp_col      = pick('signal strength (dbm)', 'rsrp', 'signal_strength', 'signal strength (dBm)')  # dataset used "Signal Strength (dBm)"
sinr_col      = pick('sinr', 'sinr_db')
throughput_col= pick('data throughput (mbps)', 'throughput', 'downlink_kbps', 'data_throughput_mbps')
latency_col   = pick('latency (ms)', 'latency', 'rtt')
jitter_col    = pick('jitter','jtr')
signal_quality_pct = pick('signal quality (%)','signal quality','signal_quality')

print("\nMapped columns:")
for name, col in [('timestamp',timestamp_col),('lat',lat_col),('lon',lon_col),
                  ('rsrp',rsrp_col),('sinr',sinr_col),('throughput',throughput_col),
                  ('latency',latency_col),('jitter',jitter_col),('signal_quality_pct',signal_quality_pct)]:
    print(f"  {name}: {col}")

# -----------------------
# 3) Basic validation & cleaning
# -----------------------
dfc = df.copy()

# parse timestamp
if timestamp_col:
    dfc[timestamp_col] = pd.to_datetime(dfc[timestamp_col], errors='coerce')

# drop exact duplicates
dfc = dfc.drop_duplicates()

# lat/lon numeric and out-of-range
if lat_col and lon_col:
    dfc[lat_col] = pd.to_numeric(dfc[lat_col], errors='coerce')
    dfc[lon_col] = pd.to_numeric(dfc[lon_col], errors='coerce')
    dfc = dfc[(dfc[lat_col].abs() <= 90) & (dfc[lon_col].abs() <= 180)]

# normalize numeric metric columns (coerce to numeric)
metric_cols = []
for c in [throughput_col, latency_col, jitter_col, rsrp_col, sinr_col, signal_quality_pct]:
    if c:
        dfc[c] = pd.to_numeric(dfc[c], errors='coerce')
        metric_cols.append(c)

# fill numeric missings with median
for c in metric_cols:
    median = dfc[c].median()
    dfc[c] = dfc[c].fillna(median)

# forward-fill timestamps if any remain null
if timestamp_col:
    dfc[timestamp_col] = dfc[timestamp_col].fillna(method='ffill')

# -----------------------
# 4) Normalization for analysis
# -----------------------
norm_cols = [c for c in [throughput_col, latency_col, jitter_col] if c in dfc.columns]
if norm_cols:
    mms = MinMaxScaler()
    dfc[[c + "_norm" for c in norm_cols]] = mms.fit_transform(dfc[norm_cols])

# --------------
# 5) Compute QoS score
# --------------
def make_qos(row):
    tp = row.get(throughput_col + "_norm", 0) if throughput_col else 0
    lat = row.get(latency_col + "_norm", 0) if latency_col else 0
    jit = row.get(jitter_col + "_norm", 0) if jitter_col else 0
    # higher throughput better; higher latency/jitter worse
    return tp - lat - jit

if norm_cols:
    dfc['qos_score'] = dfc.apply(make_qos, axis=1)
    dfc['qos_score_norm'] = MinMaxScaler().fit_transform(dfc[['qos_score']])

# -----------------------
# 6) Categorize signal quality (labels) for reporting / ML
# -----------------------
def label_signal(row):
    # Prefer RSRP-like column if available (dBm negative values typical)
    if rsrp_col and rsrp_col in row and not pd.isna(row[rsrp_col]):
        rsrp = row[rsrp_col]
        if rsrp >= -80:
            return "Excellent"
        elif rsrp >= -95:
            return "Good"
        elif rsrp >= -105:
            return "Fair"
        else:
            return "Poor"
    # fallback to signal_quality_pct if present
    if signal_quality_pct and signal_quality_pct in row:
        v = row[signal_quality_pct]
        # percent thresholds arbitrary: 75+ excellent, 50-75 good, 25-50 fair else poor
        if v >= 75:
            return "Excellent"
        elif v >= 50:
            return "Good"
        elif v >= 25:
            return "Fair"
        else:
            return "Poor"
    # fallback to qos_score_norm
    if 'qos_score_norm' in row and not pd.isna(row['qos_score_norm']):
        v = row['qos_score_norm']
        if v >= 0.75:
            return "Excellent"
        elif v >= 0.5:
            return "Good"
        elif v >= 0.25:
            return "Fair"
        else:
            return "Poor"
    return "Unknown"

dfc['signal_label'] = dfc.apply(label_signal, axis=1)

# -----------------------
# 7) Visualizations (matplotlib only)
# -----------------------
# Throughput histogram
if throughput_col:
    plt.figure(figsize=(6,4))
    plt.title("Throughput distribution")
    plt.hist(dfc[throughput_col], bins=40)
    plt.xlabel("Throughput (as in dataset)")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

# Latency histogram
if latency_col:
    plt.figure(figsize=(6,4))
    plt.title("Latency distribution")
    plt.hist(dfc[latency_col], bins=40)
    plt.xlabel("Latency (ms)")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

# Throughput over time
if timestamp_col and throughput_col:
    tmp = dfc.dropna(subset=[timestamp_col, throughput_col]).sort_values(timestamp_col)
    if not tmp.empty:
        plt.figure(figsize=(8,3))
        plt.title("Throughput over time")
        plt.plot(tmp[timestamp_col], tmp[throughput_col])
        plt.xlabel("Time")
        plt.ylabel("Throughput")
        plt.tight_layout()
        plt.show()

# Coverage scatter colored by qos_score_norm (or label)
if lat_col and lon_col:
    plt.figure(figsize=(6,6))
    plt.title("Coverage scatter (lat/lon) colored by qos_score_norm")
    if 'qos_score_norm' in dfc.columns:
        plt.scatter(dfc[lon_col], dfc[lat_col], s=8, c=dfc['qos_score_norm'])
        plt.colorbar(label='QoS (normalized)')
    else:
        mapping = {'Excellent':3,'Good':2,'Fair':1,'Poor':0,'Unknown':-1}
        vals = dfc['signal_label'].map(mapping)
        plt.scatter(dfc[lon_col], dfc[lat_col], s=8, c=vals)
        plt.colorbar()
    plt.xlabel("Longitude")
    plt.ylabel("Latitude")
    plt.tight_layout()
    plt.show()

# -----------------------
# 8) Simple ML: classify signal_label
# -----------------------
# Prepare features: numeric metrics (throughput, latency, rsrp, sinr, qos_score_norm)
features = []
for c in [throughput_col, latency_col, jitter_col, rsrp_col, sinr_col]:
    if c and c in dfc.columns:
        features.append(c)
# also include qos_score_norm if present
if 'qos_score_norm' in dfc.columns:
    features.append('qos_score_norm')

print("Features used for ML:", features)

# Only proceed if we have at least 2 features
if len(features) >= 1:
    X = dfc[features].fillna(0).values
    y = dfc['signal_label'].values

    # convert labels to simple classes (drop 'Unknown' rows)
    mask = y != 'Unknown'
    X = X[mask]
    y = y[mask]

    if len(np.unique(y)) > 1 and X.shape[0] >= 50:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
        clf = RandomForestClassifier(n_estimators=100, random_state=42)
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
        print("\nClassification report:")
        print(classification_report(y_test, y_pred))
        print("Confusion matrix:")
        print(confusion_matrix(y_test, y_pred))
    else:
        print("Not enough variety or rows to train a meaningful classifier (need >=2 classes and ~50 rows).")
else:
    print("No numeric features available to train ML model.")

# -----------------------
# 9) Save cleaned CSV and summary
# -----------------------
cleaned_csv = os.path.join(OUT_DIR, "signal_metrics_cleaned.csv")
dfc.to_csv(cleaned_csv, index=False)
report_txt = os.path.join(OUT_DIR, "summary_report.txt")
with open(report_txt, "w") as f:
    f.write("Summary report - Mobile Network Coverage & QoS\n")
    f.write(f"Original rows: {len(df)}\n")
    f.write(f"Cleaned rows: {len(dfc)}\n\n")
    f.write("Signal label counts:\n")
    f.write(dfc['signal_label'].value_counts().to_string())
    f.write("\n\nColumns mapped:\n")
    for name, col in [('timestamp',timestamp_col),('lat',lat_col),('lon',lon_col),
                      ('rsrp',rsrp_col),('sinr',sinr_col),('throughput',throughput_col),
                      ('latency',latency_col),('jitter',jitter_col),('signal_quality_pct',signal_quality_pct)]:
        f.write(f"{name}: {col}\n")
    f.write("\nSaved cleaned CSV: " + cleaned_csv + "\n")

print("\nSaved cleaned CSV:", cleaned_csv)
print("Saved report:", report_txt)

!pip install reportlab

# Install required packages for Google Colab
try:
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    import gradio as gr
except ImportError:
    print("Installing required packages...")
    !pip install gradio pandas matplotlib seaborn reportlab pillow
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    import gradio as gr

from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter
import os
import io
import base64
from PIL import Image as PILImage
import numpy as np

# Google Colab specific settings
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
plt.ioff()  # Turn off interactive plotting

# Load and validate data
try:
    # Try different possible locations in Colab
    possible_paths = [
        "signal_metrics_updated.csv",
        "/content/signal_metrics_updated.csv",
        "/content/drive/MyDrive/signal_metrics_updated.csv"  # If using Google Drive
    ]

    df = None
    for path in possible_paths:
        try:
            df = pd.read_csv(path).dropna().reset_index(drop=True)
            print(f"‚úÖ Data loaded successfully from {path}. Shape: {df.shape}")
            break
        except FileNotFoundError:
            continue

    if df is None:
        raise FileNotFoundError("CSV not found in any expected location")

except FileNotFoundError:
    # Create sample data if CSV doesn't exist
    print("üìù CSV not found. Creating sample data for demonstration...")
    np.random.seed(42)
    df = pd.DataFrame({
        'Signal Strength (dBm)': np.random.normal(-70, 15, 1000),
        'Data Throughput (Mbps)': np.random.exponential(25, 1000),
        'Latency (ms)': np.random.gamma(2, 10, 1000),
        'Signal Quality (%)': np.random.beta(4, 1, 1000) * 100
    })
    print(f"‚úÖ Sample data created for demonstration. Shape: {df.shape}")

# Create output directory in Colab's content folder
output_dir = "/content/outputs"
os.makedirs(output_dir, exist_ok=True)
print(f"üìÅ Output directory created: {output_dir}")

# ========================
# Fixed Visualization function
# ========================
def generate_visualizations():
    plots = []

    try:
        # Clear any existing plots
        plt.close('all')  # Close all existing figures

        # 1. Histogram - Fixed path and error handling
        hist_path = os.path.join(output_dir, "histogram.png")
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.hist(df['Signal Strength (dBm)'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        ax.set_title("Signal Strength Distribution", fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel("Signal Strength (dBm)", fontsize=12)
        ax.set_ylabel("Frequency", fontsize=12)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(hist_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close(fig)
        plots.append(hist_path)
        print(f"Generated histogram: {hist_path}")

        # 2. Line Graph - Fixed sampling and styling
        line_path = os.path.join(output_dir, "linegraph.png")
        fig, ax = plt.subplots(figsize=(12, 6))
        sample_size = min(100, len(df))  # Handle datasets smaller than 100
        ax.plot(df['Data Throughput (Mbps)'].head(sample_size),
               linewidth=2, color='green', alpha=0.8, marker='o', markersize=3)
        ax.set_title(f"Throughput Trend (first {sample_size} samples)", fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel("Sample Index", fontsize=12)
        ax.set_ylabel("Throughput (Mbps)", fontsize=12)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(line_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close(fig)
        plots.append(line_path)
        print(f"Generated line graph: {line_path}")

        # 3. Heatmap - Fixed correlation calculation
        heatmap_path = os.path.join(output_dir, "heatmap.png")
        fig, ax = plt.subplots(figsize=(10, 8))

        # Select only numeric columns for correlation
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
        correlation_matrix = df[numeric_cols].corr()

        sns.heatmap(correlation_matrix,
                   annot=True,
                   cmap="RdYlBu_r",
                   center=0,
                   square=True,
                   fmt='.2f',
                   cbar_kws={'shrink': 0.8},
                   ax=ax)
        ax.set_title("Feature Correlation Heatmap", fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        plt.savefig(heatmap_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close(fig)
        plots.append(heatmap_path)
        print(f"Generated heatmap: {heatmap_path}")

        # 4. Scatter plot for better insights
        scatter_path = os.path.join(output_dir, "scatter.png")
        fig, ax = plt.subplots(figsize=(12, 8))
        scatter = ax.scatter(df['Signal Strength (dBm)'], df['Data Throughput (Mbps)'],
                   alpha=0.6, c=df['Signal Quality (%)'], cmap='viridis', s=50)
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('Signal Quality (%)', fontsize=12)
        ax.set_title("Signal Strength vs Throughput (colored by Quality)", fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel("Signal Strength (dBm)", fontsize=12)
        ax.set_ylabel("Data Throughput (Mbps)", fontsize=12)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(scatter_path, dpi=150, bbox_inches='tight', facecolor='white')
        plt.close(fig)
        plots.append(scatter_path)
        print(f"Generated scatter plot: {scatter_path}")

        # Verify all files were created successfully
        valid_plots = []
        for plot_path in plots:
            if os.path.exists(plot_path) and os.path.getsize(plot_path) > 1000:  # At least 1KB
                valid_plots.append(plot_path)
            else:
                print(f"Warning: Plot file issue - {plot_path}")

        print(f"‚úÖ Generated {len(valid_plots)}/{len(plots)} valid visualizations")
        return valid_plots

    except Exception as e:
        print(f"‚ùå Error in generate_visualizations: {str(e)}")
        import traceback
        traceback.print_exc()
        return []

# ========================
# Fixed Report Generation
# ========================
def generate_report(user_note=""):
    try:
        report_path = os.path.join(output_dir, "network_report.pdf")
        doc = SimpleDocTemplate(report_path, pagesize=letter)
        styles = getSampleStyleSheet()
        story = []

        # Title
        story.append(Paragraph("Mobile Network Coverage & QoS Report", styles['Title']))
        story.append(Spacer(1, 20))

        # Executive Summary
        story.append(Paragraph("Executive Summary", styles['Heading1']))
        avg_signal = df['Signal Strength (dBm)'].mean()
        avg_throughput = df['Data Throughput (Mbps)'].mean()
        avg_latency = df['Latency (ms)'].mean()
        avg_quality = df['Signal Quality (%)'].mean()

        summary_text = f"""
        This report analyzes {len(df)} network measurement samples.
        The average signal strength is {avg_signal:.1f} dBm, with an average
        throughput of {avg_throughput:.1f} Mbps and latency of {avg_latency:.1f} ms.
        Overall signal quality averages {avg_quality:.1f}%.
        """
        story.append(Paragraph(summary_text, styles['Normal']))
        story.append(Spacer(1, 12))

        # Dataset Statistics Table - Fixed HTML issue
        story.append(Paragraph("Dataset Statistics", styles['Heading2']))

        # Create proper table instead of HTML
        stats_data = []
        stats_data.append(['Metric', 'Mean', 'Std Dev', 'Min', 'Max'])

        for col in df.select_dtypes(include=['float64', 'int64']).columns:
            stats_data.append([
                col,
                f"{df[col].mean():.2f}",
                f"{df[col].std():.2f}",
                f"{df[col].min():.2f}",
                f"{df[col].max():.2f}"
            ])

        stats_table = Table(stats_data)
        stats_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 12),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(stats_table)
        story.append(Spacer(1, 20))

        # QoS Analysis
        story.append(Paragraph("Quality of Service Analysis", styles['Heading2']))

        # Performance categorization
        good_signal_pct = (df['Signal Strength (dBm)'] > -75).mean() * 100
        high_throughput_pct = (df['Data Throughput (Mbps)'] > 20).mean() * 100
        low_latency_pct = (df['Latency (ms)'] < 50).mean() * 100

        qos_text = f"""
        Performance Metrics:<br/>
        ‚Ä¢ {good_signal_pct:.1f}% of measurements have good signal strength (> -75 dBm)<br/>
        ‚Ä¢ {high_throughput_pct:.1f}% of measurements achieve high throughput (> 20 Mbps)<br/>
        ‚Ä¢ {low_latency_pct:.1f}% of measurements have low latency (< 50 ms)<br/>
        ‚Ä¢ Average signal quality: {avg_quality:.1f}%<br/>
        """
        story.append(Paragraph(qos_text, styles['Normal']))
        story.append(Spacer(1, 12))

        # User notes
        if user_note and user_note.strip():
            story.append(Paragraph("User Notes", styles['Heading2']))
            story.append(Paragraph(user_note.strip(), styles['Normal']))
            story.append(Spacer(1, 12))

        # Generate and add plots
        story.append(Paragraph("Data Visualizations", styles['Heading2']))
        plots = generate_visualizations()

        for i, plot_path in enumerate(plots):
            if os.path.exists(plot_path):
                try:
                    # Add some space before image
                    story.append(Spacer(1, 12))

                    # Add image with proper sizing
                    img = Image(plot_path, width=450, height=300)  # Fixed sizing
                    story.append(img)
                    story.append(Spacer(1, 6))

                except Exception as e:
                    print(f"Error adding image {plot_path}: {str(e)}")
                    error_text = f"Error loading visualization {i+1}"
                    story.append(Paragraph(error_text, styles['Normal']))

        # Build the PDF
        doc.build(story)

        # Verify file was created
        if os.path.exists(report_path):
            print(f"Report generated successfully: {report_path}")
            return report_path
        else:
            raise Exception("PDF file was not created")

    except Exception as e:
        print(f"Error in generate_report: {str(e)}")
        # Return error message file
        error_path = os.path.join(output_dir, "error_report.txt")
        with open(error_path, 'w') as f:
            f.write(f"Error generating report: {str(e)}")
        return error_path

# ========================
# Enhanced Gradio Interface
# ========================
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üì∂ Mobile Network Coverage & QoS Analyzer")
    gr.Markdown("Analyze mobile network performance metrics with advanced visualizations and reporting.")

    with gr.Tab("üìä Data Preview"):
        with gr.Row():
            with gr.Column():
                n_rows = gr.Slider(1, 50, value=10, step=1, label="Rows to display")
                refresh_btn = gr.Button("Refresh Data", variant="secondary")
            with gr.Column():
                data_info = gr.Textbox(
                    value=f"Dataset shape: {df.shape}",
                    label="Dataset Info",
                    interactive=False
                )

        data_output = gr.Dataframe(value=df.head(10), interactive=False)

        # Update functions
        def update_data_preview(n):
            return df.head(n)

        def refresh_data_info():
            return f"Dataset shape: {df.shape}\nColumns: {', '.join(df.columns)}\nMemory usage: {df.memory_usage(deep=True).sum() / 1024:.1f} KB"

        n_rows.change(update_data_preview, inputs=n_rows, outputs=data_output)
        refresh_btn.click(refresh_data_info, outputs=data_info)

    with gr.Tab("üìà Visualization"):
        gr.Markdown("Generate comprehensive visualizations of your network data.")

        with gr.Row():
            btn_viz = gr.Button("üé® Generate Visualizations", variant="primary", size="lg")
            clear_btn = gr.Button("üóëÔ∏è Clear", variant="secondary")

        viz_status = gr.Textbox(label="Status", interactive=False)
        gallery = gr.Gallery(
            label="Generated Plots",
            columns=2,
            rows=2,
            height=600,
            object_fit="contain",
            show_label=True,
            elem_id="plot_gallery"
        )

        def generate_viz_with_status():
            try:
                plots = generate_visualizations()
                if plots:
                    # Verify all plots exist and are accessible
                    valid_plots = []
                    for plot_path in plots:
                        if os.path.exists(plot_path):
                            # Check file size to ensure it's not empty
                            if os.path.getsize(plot_path) > 0:
                                valid_plots.append(plot_path)
                                print(f"‚úÖ Valid plot: {plot_path} ({os.path.getsize(plot_path)} bytes)")
                            else:
                                print(f"‚ö†Ô∏è Empty plot file: {plot_path}")
                        else:
                            print(f"‚ùå Plot not found: {plot_path}")

                    if valid_plots:
                        status = f"‚úÖ Successfully generated {len(valid_plots)}/{len(plots)} visualizations"
                        # Return the list of valid plot paths
                        return valid_plots, status
                    else:
                        return [], "‚ùå No valid visualizations generated"
                else:
                    return [], "‚ùå No visualizations generated"
            except Exception as e:
                print(f"Error in generate_viz_with_status: {str(e)}")
                return [], f"‚ùå Error: {str(e)}"

        def clear_gallery():
            return [], "Gallery cleared"

        btn_viz.click(generate_viz_with_status, outputs=[gallery, viz_status])
        clear_btn.click(clear_gallery, outputs=[gallery, viz_status])

    with gr.Tab("‚ÑπÔ∏è About Model"):
        gr.Markdown(
            """
            ## Mobile Network Coverage & QoS Analyzer

            ### üéØ Purpose
            This tool analyzes mobile network performance data to provide insights into coverage quality and service metrics.

            ### üîÑ Process Flow
            1. **Data Upload/Collection** - Load CSV data with network metrics
            2. **Data Validation & Cleaning** - Remove invalid entries and handle missing values
            3. **Normalization** - Standardize data for consistent analysis
            4. **Analysis** - Calculate QoS metrics and coverage statistics
            5. **Visualization** - Generate histograms, trends, correlations, and scatter plots
            6. **Report Generation** - Create comprehensive PDF reports with insights

            ### üìä Key Metrics
            - **Signal Strength (dBm)** - Radio signal power level
            - **Data Throughput (Mbps)** - Data transfer speed
            - **Latency (ms)** - Network response time
            - **Signal Quality (%)** - Overall connection quality

            ### üõ†Ô∏è Technologies
            - **Pandas** - Data manipulation and analysis
            - **Matplotlib/Seaborn** - Data visualization
            - **ReportLab** - PDF report generation
            - **Gradio** - Interactive web interface
            """
        )

    with gr.Tab("üìã Generate Report"):
        gr.Markdown("Create a comprehensive PDF report with analysis and visualizations.")

        # Add file upload option for Colab users
        gr.Markdown("### üìÅ Optional: Upload your own CSV file")
        file_upload = gr.File(
            label="Upload CSV file (optional)",
            file_types=[".csv"],
            type="filepath"
        )
        upload_status = gr.Textbox(label="Upload Status", interactive=False)

        def handle_file_upload(filepath):
            global df
            if filepath is not None:
                try:
                    new_df = pd.read_csv(filepath).dropna().reset_index(drop=True)

                    # Validate that it has the expected columns
                    required_cols = ['Signal Strength (dBm)', 'Data Throughput (Mbps)',
                                   'Latency (ms)', 'Signal Quality (%)']
                    missing_cols = [col for col in required_cols if col not in new_df.columns]

                    if missing_cols:
                        return f"‚ùå Missing columns: {', '.join(missing_cols)}"

                    df = new_df
                    return f"‚úÖ Data uploaded successfully! Shape: {df.shape}"
                except Exception as e:
                    return f"‚ùå Error reading file: {str(e)}"
            return "No file uploaded"

        file_upload.change(handle_file_upload, inputs=file_upload, outputs=upload_status)

        gr.Markdown("### üìù Report Generation")

        with gr.Column():
            user_note = gr.Textbox(
                label="üìù Custom Notes",
                placeholder="Add any additional comments or observations for the report...",
                lines=4
            )

            with gr.Row():
                report_btn = gr.Button("üìÑ Generate PDF Report", variant="primary", size="lg")

            report_status = gr.Textbox(label="Status", interactive=False)
            report_file = gr.File(label="üì• Download Report", height=100)

        def generate_report_with_status(notes):
            try:
                report_path = generate_report(notes)
                if os.path.exists(report_path) and report_path.endswith('.pdf'):
                    status = "‚úÖ Report generated successfully! Click download link above."
                    return report_path, status
                else:
                    status = "‚ö†Ô∏è Report generated with errors. Check the file."
                    return report_path, status
            except Exception as e:
                error_status = f"‚ùå Error generating report: {str(e)}"
                return None, error_status

        report_btn.click(
            generate_report_with_status,
            inputs=user_note,
            outputs=[report_file, report_status]
        )

# Launch the interface for Google Colab
if __name__ == "__main__":
    print("üöÄ Launching Gradio interface for Google Colab...")

    # Colab-specific launch configuration
    demo.launch(
        share=True,  # Enable public sharing for Colab
        debug=True,  # Enable debug mode for better error tracking
        show_error=True,
        height=600,  # Set height for Colab iframe
        quiet=False
    )

    print("‚úÖ Interface launched! Use the public URL above to access your app.")
    print("üí° Tip: The interface will remain active as long as this cell is running.")

import pandas as pd
import numpy as np

# Load your CSV
df = pd.read_csv("signal_metrics_cleaned.csv")

# Replace 0s in 'signal quality' with random values (0-100 range)
df['Signal Quality (%)'] = df['Signal Quality (%)'].apply(lambda x: np.random.rand() * 100 if x == 0 else x)

# Save back to CSV
df.to_csv("signal_metrics_updated.csv", index=False)



df.head()